{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "b553t2EFOuzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "Go5RiarKOwlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjIotBP5V9dQ"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "id": "JWdDWHlpHz7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import modules required\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from os import listdir\n",
        "from os.path import isfile\n",
        "from IPython.utils import capture\n",
        "from IPython.display import clear_output\n",
        "import wget\n",
        "from subprocess import check_output\n",
        "import urllib.request\n",
        "import requests\n",
        "import base64\n",
        "from gdown.download import get_url_from_gdrive_confirmation\n",
        "from urllib.parse import urlparse, parse_qs, unquote\n",
        "from urllib.request import urlopen, Request\n",
        "from subprocess import getoutput\n",
        "import shutil\n",
        "from google.colab import files, runtime\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import ipywidgets as widgets\n",
        "from io import BytesIO"
      ],
      "metadata": {
        "id": "sVGRooTvYKtu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencies\n",
        "\n",
        "print('Installing dependencies...')\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "    os.chdir('/content')\n",
        "    !pip install -qq --no-deps accelerate==0.12.0\n",
        "    !wget -q -i https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dependencies/dbdeps.txt\n",
        "    !dpkg -i *.deb\n",
        "    !tar -C / --zstd -xf gcolabdeps.tar.zst\n",
        "    !rm *.deb | rm *.zst | rm *.txt\n",
        "    !git clone -q --depth 1 --branch main https://github.com/TheLastBen/diffusers\n",
        "    !pip install gradio==3.16.2 --no-deps -qq\n",
        "\n",
        "    if not os.path.exists('gdrive/MyDrive/sd/libtcmalloc/libtcmalloc_minimal.so.4'):\n",
        "        %env CXXFLAGS=-std=c++14\n",
        "        !wget -q https://github.com/gperftools/gperftools/releases/download/gperftools-2.5/gperftools-2.5.tar.gz && tar zxf gperftools-2.5.tar.gz && mv gperftools-2.5 gperftools\n",
        "        !wget -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/AUTOMATIC1111_files/Patch\n",
        "        %cd /content/gperftools\n",
        "        !patch -p1 < /content/Patch\n",
        "        !./configure --enable-minimal --enable-libunwind --enable-frame-pointers --enable-dynamic-sized-delete-support --enable-sized-delete --enable-emergency-malloc; make -j4\n",
        "        !mkdir -p /content/gdrive/MyDrive/sd/libtcmalloc && cp .libs/libtcmalloc*.so* /content/gdrive/MyDrive/sd/libtcmalloc\n",
        "        %env LD_PRELOAD=/content/gdrive/MyDrive/sd/libtcmalloc/libtcmalloc_minimal.so.4\n",
        "        %cd /content\n",
        "        !rm *.tar.gz Patch && rm -r /content/gperftools\n",
        "    else:\n",
        "        %env LD_PRELOAD=/content/gdrive/MyDrive/sd/libtcmalloc/libtcmalloc_minimal.so.4\n",
        "\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "    os.environ['PYTHONWARNINGS'] = 'ignore'\n",
        "\n",
        "print('Installation completed successfully!')"
      ],
      "metadata": {
        "id": "XDtw1rVrWeAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================================================================================================================================================="
      ],
      "metadata": {
        "id": "eVHu7GQZWeJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Download\n",
        "# Skip this cell when loading a previous session that contains a trained model\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "    os.chdir('/content')\n",
        "\n",
        "if os.path.exists('/content/gdrive/MyDrive/Dreambooth-Training/token.txt'):\n",
        "    with open(\"/content/gdrive/MyDrive/Dreambooth-Training/token.txt\") as f:\n",
        "        token = f.read()\n",
        "    auth = f'https://USER:{token}@'\n",
        "else:\n",
        "    auth = \"https://\"\n",
        "\n",
        "\n",
        "def download_model():\n",
        "    if os.path.exists('/content/stable-diffusion-v1-5'):\n",
        "        !rm -r /content/stable-diffusion-v1-5\n",
        "\n",
        "    os.chdir('/content')\n",
        "\n",
        "    !mkdir /content/stable-diffusion-v1-5\n",
        "    os.chdir('/content/stable-diffusion-v1-5')\n",
        "    !git config --global init.defaultBranch main\n",
        "    !git init\n",
        "    !git lfs install --system --skip-repo\n",
        "    !git remote add -f origin  \"https://huggingface.co/runwayml/stable-diffusion-v1-5\"\n",
        "    !git config core.sparsecheckout true\n",
        "    !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nmodel_index.json\\n!vae/diffusion_pytorch_model.bin\\n!*.safetensors\\n!*.fp16.bin\\n!*.non_ema.bin\" > .git/info/sparse-checkout\n",
        "    !git pull origin main\n",
        "    if os.path.exists('/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
        "        !wget -q -O vae/diffusion_pytorch_model.bin https://huggingface.co/stabilityai/sd-vae-ft-mse/resolve/main/diffusion_pytorch_model.bin\n",
        "        !rm -r .git\n",
        "        !rm model_index.json\n",
        "        time.sleep(1)\n",
        "        wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n",
        "        os.chdir('/content')\n",
        "        print('Done!')\n",
        "    else:\n",
        "        while not os.path.exists('/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
        "            print('Something went wrong...')\n",
        "            time.sleep(5)\n",
        "\n",
        "\n",
        "if not os.path.exists('/content/stable-diffusion-v1-5'):\n",
        "    download_model()\n",
        "    MODEL_NAME = \"/content/stable-diffusion-v1-5\"\n",
        "else:\n",
        "    MODEL_NAME = \"/content/stable-diffusion-v1-5\"\n",
        "    print(\"The v1.5 model already exists, using this model.\")"
      ],
      "metadata": {
        "id": "ENBv8PkVWeRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================================================================================================================================================="
      ],
      "metadata": {
        "id": "dsbFnxN4kdFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create/Load a Session\n",
        "\n",
        "# ==================================================================================================\n",
        "# Enter the session name.\n",
        "# If an older session with the same name exists, that will be loaded. Otherwise a new session will be made.\n",
        "Session_Name = 'comicpanel2'\n",
        "\n",
        "\n",
        "try:\n",
        "    MODEL_NAME\n",
        "    pass\n",
        "except:\n",
        "    MODEL_NAME = \"\"\n",
        "\n",
        "PT = \"\"\n",
        "\n",
        "while Session_Name == \"\":\n",
        "    print('Input the Session Name:')\n",
        "    Session_Name = input('')\n",
        "Session_Name = Session_Name.replace(\" \", \"_\")\n",
        "\n",
        "\n",
        "INSTANCE_NAME = Session_Name\n",
        "OUTPUT_DIR = \"/content/models/\" + Session_Name\n",
        "SESSION_DIR = '/content/gdrive/MyDrive/Dreambooth-Training/Sessions/' + Session_Name\n",
        "INSTANCE_DIR = SESSION_DIR + '/instance_images'\n",
        "CAPTIONS_DIR = SESSION_DIR + '/captions'\n",
        "MDLPTH = str(SESSION_DIR + \"/\" + Session_Name + '.ckpt')\n",
        "\n",
        "if os.path.exists(str(SESSION_DIR)):\n",
        "    mdls = [ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1] == \"ckpt\"]\n",
        "\n",
        "    if not os.path.exists(MDLPTH) and '.ckpt' in str(mdls):\n",
        "        def f(n):\n",
        "            k = 0\n",
        "            for i in mdls:\n",
        "                if k == n:\n",
        "                    !mv \"$SESSION_DIR/$i\" $MDLPTH\n",
        "                k += 1\n",
        "\n",
        "        k = 0\n",
        "        print('No final checkpoint model found, select which intermediary checkpoint to use, enter only the number, (000 to skip):\\n')\n",
        "\n",
        "        for i in mdls:\n",
        "            print(str(k) + '-', i)\n",
        "            k += 1\n",
        "        n = input()\n",
        "\n",
        "        while int(n) > k-1:\n",
        "            n = input()\n",
        "        if n != \"000\":\n",
        "            f(int(n))\n",
        "            print('Using the model', mdls[int(n)] + \"...\")\n",
        "            time.sleep(2)\n",
        "        else:\n",
        "            print('Skipping the intermediary checkpoints...')\n",
        "        del n\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "    %cd /content\n",
        "    resume = False\n",
        "\n",
        "if os.path.exists(str(SESSION_DIR)) and not os.path.exists(MDLPTH):\n",
        "    print('Loading session with no previous model, using the original model or the custom downloaded model')\n",
        "    if MODEL_NAME == \"\":\n",
        "        print('No model found, use the \"Model Download\" cell to download a model.')\n",
        "    else:\n",
        "        print('Session Loaded, proceed to uploading instance images')\n",
        "\n",
        "elif os.path.exists(MDLPTH):\n",
        "    print('Session found, loading the trained model ...')\n",
        "    wget.download('https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/det.py')\n",
        "    print('Detecting model version...')\n",
        "    Model_Version = check_output('python det.py --MODEL_PATH ' + MDLPTH, shell=True).decode('utf-8').replace('\\n', '')\n",
        "\n",
        "    print(Model_Version + ' Detected')\n",
        "    !rm det.py\n",
        "    !wget -q -O config.yaml https://github.com/CompVis/stable-diffusion/raw/main/configs/stable-diffusion/v1-inference.yaml\n",
        "    print('Session found, loading the trained model ...')\n",
        "    !python /content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path $MDLPTH --dump_path \"$OUTPUT_DIR\" --original_config_file config.yaml\n",
        "    !rm /content/config.yaml\n",
        "\n",
        "\n",
        "    if os.path.exists(OUTPUT_DIR + '/unet/diffusion_pytorch_model.bin'):\n",
        "        resume=True\n",
        "        clear_output()\n",
        "        print('Session loaded.')\n",
        "    else:\n",
        "        if not os.path.exists(OUTPUT_DIR + '/unet/diffusion_pytorch_model.bin'):\n",
        "            print('Conversion error, if the error persists, remove the CKPT file from the current session folder')\n",
        "\n",
        "elif not os.path.exists(str(SESSION_DIR)):\n",
        "    %mkdir -p \"$INSTANCE_DIR\"\n",
        "    print('Creating session...')\n",
        "    if MODEL_NAME == \"\":\n",
        "        print('No model found, use the \"Model Download\" cell to download a model.')\n",
        "    else:\n",
        "        print('Session created, proceed to uploading instance images')"
      ],
      "metadata": {
        "id": "d7xCKEhwWeZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================================================================================================================================================="
      ],
      "metadata": {
        "id": "CTn_sTUQmuNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instance Images\n",
        "\n",
        "# Set to False to keep any existing instance images.\n",
        "remove_existing_instance_images = False\n",
        "\n",
        "# instance images will be taken from the directory specified.\n",
        "images_folder = '/content/gdrive/MyDrive/Uni Work/third year/FYP/training data/panels/sep'\n",
        "\n",
        "# Make sure to upload the captions separately into the captions folder.\n",
        "\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "    %cd /content\n",
        "\n",
        "\n",
        "if remove_existing_instance_images:\n",
        "    if os.path.exists(str(INSTANCE_DIR)):\n",
        "        !rm -r \"$INSTANCE_DIR\"\n",
        "    if os.path.exists(str(CAPTIONS_DIR)):\n",
        "        !rm -r \"$CAPTIONS_DIR\"\n",
        "\n",
        "if not os.path.exists(str(INSTANCE_DIR)):\n",
        "    %mkdir -p \"$INSTANCE_DIR\"\n",
        "if not os.path.exists(str(CAPTIONS_DIR)):\n",
        "    %mkdir -p \"$CAPTIONS_DIR\"\n",
        "\n",
        "if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n",
        "    %rm -r $INSTANCE_DIR\"/.ipynb_checkpoints\"\n",
        "\n",
        "\n",
        "\n",
        "while images_folder != \"\" and not os.path.exists(str(images_folder)):\n",
        "    print('The images folder specified does not exist, copy the path in here:')\n",
        "    images_folder = input('')\n",
        "\n",
        "\n",
        "if os.path.exists(images_folder + \"/.ipynb_checkpoints\"):\n",
        "    %rm -r \"$images_folder\"\"/.ipynb_checkpoints\"\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "    !mv $images_folder/*.txt $CAPTIONS_DIR\n",
        "for filename in tqdm(os.listdir(images_folder), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
        "    %cp -r \"$images_folder/$filename\" \"$INSTANCE_DIR\"\n",
        "\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "    %cd \"$INSTANCE_DIR\"\n",
        "    !find . -name \"* *\" -type f | rename 's/ /-/g'\n",
        "    %cd \"$CAPTIONS_DIR\"\n",
        "    !find . -name \"* *\" -type f | rename 's/ /-/g'\n",
        "\n",
        "    %cd $SESSION_DIR\n",
        "    !rm instance_images.zip captions.zip\n",
        "    !zip -r instance_images instance_images\n",
        "    !zip -r captions captions\n",
        "    %cd /content\n",
        "\n",
        "print('\\nUploading images complete!')"
      ],
      "metadata": {
        "id": "rfHMLJ_jWehW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================================================================================================================================================="
      ],
      "metadata": {
        "id": "LQ6TCKj3ornL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "#Start DreamBooth\n",
        "\n",
        "\n",
        "# Set this to True to continue training the prior model.\n",
        "Resume_Training = False\n",
        "\n",
        "# ==============\n",
        "# Training parameters\n",
        "\n",
        "UNet_Training_Steps = 3000\n",
        "\n",
        "# UNet_Learning_Rate = 2e-6\n",
        "UNet_Learning_Rate = 1e-5\n",
        "untlr = UNet_Learning_Rate\n",
        "\n",
        "Text_Encoder_Training_Steps = 900\n",
        "\n",
        "# keep low to avoid overfitting (1e-6 is higher than 4e-7)\n",
        "# Text_Encoder_Learning_Rate = 1e-6\n",
        "Text_Encoder_Learning_Rate = 2e-5\n",
        "txlr = Text_Encoder_Learning_Rate\n",
        "\n",
        "# Always set as True for style training. Not needed for faces.\n",
        "Offset_Noise = True\n",
        "# ==============\n",
        "\n",
        "\n",
        "\n",
        "if os.path.exists(INSTANCE_DIR + \"/.ipynb_checkpoints\"):\n",
        "    %rm -r $INSTANCE_DIR\"/.ipynb_checkpoints\"\n",
        "\n",
        "if os.path.exists(CAPTIONS_DIR+\"/.ipynb_checkpoints\"):\n",
        "    %rm -r $CAPTIONS_DIR\"/.ipynb_checkpoints\"\n",
        "\n",
        "\n",
        "if resume and not Resume_Training:\n",
        "    print('Overwrite previously trained model? (\"yes\"/\"no\") - \"no\" will resume training the prior model.')\n",
        "    while True:\n",
        "        ansres = input('')\n",
        "        if ansres == 'no':\n",
        "            Resume_Training = True\n",
        "            break\n",
        "        elif ansres == 'yes':\n",
        "            Resume_Training = False\n",
        "            resume = False\n",
        "            break\n",
        "\n",
        "while not Resume_Training and MODEL_NAME == \"\":\n",
        "    print('No model found, go back and download a model.')\n",
        "    time.sleep(5)\n",
        "\n",
        "MODELT_NAME = MODEL_NAME\n",
        "\n",
        "\n",
        "trnonltxt = \"\"\n",
        "if UNet_Training_Steps == 0:\n",
        "    trnonltxt = \"--train_only_text_encoder\"\n",
        "\n",
        "ofstnse = \"\"\n",
        "if Offset_Noise:\n",
        "    ofstnse = \"--offset_noise\"\n",
        "\n",
        "Seed = random.randint(1, 999999)\n",
        "resuming = \"\"\n",
        "\n",
        "if Resume_Training and os.path.exists(OUTPUT_DIR + '/unet/diffusion_pytorch_model.bin'):\n",
        "    MODELT_NAME = OUTPUT_DIR\n",
        "    print('Resuming Training...')\n",
        "    resuming = \"Yes\"\n",
        "elif Resume_Training and not os.path.exists(OUTPUT_DIR + '/unet/diffusion_pytorch_model.bin'):\n",
        "    print('Previous model not found, training a new model...')\n",
        "    MODELT_NAME = MODEL_NAME\n",
        "    while MODEL_NAME == \"\":\n",
        "        print('No model found, use the \"Model Download\" cell to download a model.')\n",
        "        time.sleep(5)\n",
        "\n",
        "\n",
        "TexRes = 512\n",
        "GCUNET = \"\"\n",
        "\n",
        "\n",
        "Enable_text_encoder_training = True\n",
        "\n",
        "if Text_Encoder_Training_Steps == 0:\n",
        "    Enable_text_encoder_training = False\n",
        "else:\n",
        "    stptxt = Text_Encoder_Training_Steps\n",
        "\n",
        "\n",
        "def dump_only_textenc(trnonltxt, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps):\n",
        "    !accelerate launch /content/diffusers/examples/dreambooth/train_dreambooth.py \\\n",
        "    $trnonltxt \\\n",
        "    --external_captions \\\n",
        "    $ofstnse \\\n",
        "    --image_captions_filename \\\n",
        "    --train_text_encoder \\\n",
        "    --dump_only_text_encoder \\\n",
        "    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n",
        "    --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
        "    --output_dir=\"$OUTPUT_DIR\" \\\n",
        "    --captions_dir=\"$CAPTIONS_DIR\" \\\n",
        "    --instance_prompt=\"$PT\" \\\n",
        "    --seed=$Seed \\\n",
        "    --resolution=$TexRes \\\n",
        "    --mixed_precision=$precision \\\n",
        "    --train_batch_size=1 \\\n",
        "    --gradient_accumulation_steps=1 --gradient_checkpointing \\\n",
        "    --use_8bit_adam \\\n",
        "    --learning_rate=$txlr \\\n",
        "    --lr_scheduler=\"linear\" \\\n",
        "    --lr_warmup_steps=0 \\\n",
        "    --max_train_steps=$Training_Steps\n",
        "\n",
        "def train_only_unet(SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps):\n",
        "    if resuming == \"Yes\":\n",
        "        print('Resuming Training...')\n",
        "    print('Training the UNet...')\n",
        "    !accelerate launch /content/diffusers/examples/dreambooth/train_dreambooth.py \\\n",
        "    --external_captions \\\n",
        "    $ofstnse \\\n",
        "    --image_captions_filename \\\n",
        "    --train_only_unet \\\n",
        "    --Session_dir=$SESSION_DIR \\\n",
        "    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n",
        "    --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
        "    --output_dir=\"$OUTPUT_DIR\" \\\n",
        "    --captions_dir=\"$CAPTIONS_DIR\" \\\n",
        "    --instance_prompt=\"$PT\" \\\n",
        "    --seed=$Seed \\\n",
        "    --resolution=$Res \\\n",
        "    --mixed_precision=$precision \\\n",
        "    --train_batch_size=1 \\\n",
        "    --gradient_accumulation_steps=1 $GCUNET \\\n",
        "    --use_8bit_adam \\\n",
        "    --learning_rate=$untlr \\\n",
        "    --lr_scheduler=\"linear\" \\\n",
        "    --lr_warmup_steps=0 \\\n",
        "    --max_train_steps=$Training_Steps\n",
        "\n",
        "\n",
        "if Enable_text_encoder_training :\n",
        "    print('Training the text encoder...')\n",
        "    if os.path.exists(OUTPUT_DIR + '/' + 'text_encoder_trained'):\n",
        "        %rm -r $OUTPUT_DIR\"/text_encoder_trained\"\n",
        "    dump_only_textenc(trnonltxt, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, 'fp16', Training_Steps=stptxt)\n",
        "\n",
        "\n",
        "if UNet_Training_Steps != 0:\n",
        "    train_only_unet(SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, 512, 'fp16', Training_Steps=UNet_Training_Steps)\n",
        "\n",
        "if UNet_Training_Steps == 0 and Text_Encoder_Training_Steps == 0:\n",
        "    print('All training steps were set to 0, there is nothing to do.')\n",
        "else:\n",
        "    if os.path.exists('/content/models/' + INSTANCE_NAME + '/unet/diffusion_pytorch_model.bin'):\n",
        "        prc = \"--fp16\"\n",
        "        !python /content/diffusers/scripts/convertosdv2.py $prc $OUTPUT_DIR $SESSION_DIR/$Session_Name\".ckpt\"\n",
        "\n",
        "        filepath = SESSION_DIR + '/' + INSTANCE_NAME + '.ckpt'\n",
        "        if os.path.exists(filepath):\n",
        "            print(\"Training complete! Trained checkpoint model available at\", filepath)\n",
        "        else:\n",
        "            print(\"Something went wrong, trained model instance not created.\")\n",
        "    else:\n",
        "        print(\"Something went wrong!\")"
      ],
      "metadata": {
        "id": "J8wRjJ4FWeyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================================================================================================================================================="
      ],
      "metadata": {
        "id": "m4wzVVWrDlQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the Trained Model to HuggingFace.\n",
        "Name_of_the_concept = ''\n",
        "\n",
        "# Create write-access token: https://huggingface.co/settings/tokens\n",
        "# \"New token\" > Role: Write. Read tokens won't work.\n",
        "hf_token = ''\n",
        "\n",
        "\n",
        "# imports for uploading to huggingface\n",
        "from slugify import slugify\n",
        "from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "from huggingface_hub import create_repo\n",
        "from IPython.display import display_markdown\n",
        "from IPython.display import clear_output\n",
        "from IPython.utils import capture\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "if (Name_of_the_concept == \"\"):\n",
        "    Name_of_the_concept = Session_Name\n",
        "Name_of_the_concept = Name_of_the_concept.replace(\" \", \"-\")\n",
        "\n",
        "\n",
        "api = HfApi()\n",
        "your_username = api.whoami(token=hf_token)[\"name\"]\n",
        "\n",
        "repo_id = f\"{your_username}/{slugify(Name_of_the_concept)}\"\n",
        "output_dir = f'/content/models/' + INSTANCE_NAME\n",
        "\n",
        "def bar(prg):\n",
        "    return \"Uploading to HuggingFace : \" '|'+'â–ˆ' * prg + ' ' * (25 - prg) + '| ' + str(prg * 4) + \"%\"\n",
        "\n",
        "\n",
        "print(\"Loading...\")\n",
        "\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "    %cd $OUTPUT_DIR\n",
        "    !rm -r safety_checker feature_extractor .git\n",
        "    !rm model_index.json\n",
        "    !git init\n",
        "    !git lfs install --system --skip-repo\n",
        "    !git remote add -f origin  \"https://USER:{hf_token}@huggingface.co/runwayml/stable-diffusion-v1-5\"\n",
        "    !git config core.sparsecheckout true\n",
        "    !echo -e \"feature_extractor\\nsafety_checker\\nmodel_index.json\" > .git/info/sparse-checkout\n",
        "    !git pull origin main\n",
        "    !rm -r .git\n",
        "    %cd /content\n",
        "\n",
        "\n",
        "image_string = \"\"\n",
        "\n",
        "readme_text = f'''---\n",
        "license: creativeml-openrail-m\n",
        "tags:\n",
        "- text-to-image\n",
        "- stable-diffusion\n",
        "---\n",
        "## {Name_of_the_concept} - Diffusion model trained on Stable Diffusion 1.5\n",
        "Trigger-word: \"\"\n",
        "'''\n",
        "\n",
        "# Write Readme.md to a file\n",
        "readme_file = open(\"README.md\", \"w\")\n",
        "readme_file.write(readme_text)\n",
        "readme_file.close()\n",
        "\n",
        "\n",
        "operations = [\n",
        "    CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "    CommitOperationAdd(path_in_repo=f\"{Session_Name}.ckpt\",path_or_fileobj=MDLPTH)\n",
        "]\n",
        "create_repo(repo_id,private=True, token=hf_token)\n",
        "\n",
        "api.create_commit(\n",
        "    repo_id=repo_id,\n",
        "    operations=operations,\n",
        "    commit_message=f\"Upload the concept {Name_of_the_concept} embeds and token\",\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=OUTPUT_DIR + \"/feature_extractor\",\n",
        "    path_in_repo=\"feature_extractor\",\n",
        "    repo_id=repo_id,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "print(bar(4))\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=OUTPUT_DIR + \"/safety_checker\",\n",
        "    path_in_repo=\"safety_checker\",\n",
        "    repo_id=repo_id,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "print(bar(8))\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=OUTPUT_DIR + \"/scheduler\",\n",
        "    path_in_repo=\"scheduler\",\n",
        "    repo_id=repo_id,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "print(bar(9))\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=OUTPUT_DIR + \"/text_encoder\",\n",
        "    path_in_repo=\"text_encoder\",\n",
        "    repo_id=repo_id,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "print(bar(12))\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=OUTPUT_DIR + \"/tokenizer\",\n",
        "    path_in_repo=\"tokenizer\",\n",
        "    repo_id=repo_id,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "print(bar(13))\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=OUTPUT_DIR + \"/unet\",\n",
        "    path_in_repo=\"unet\",\n",
        "    repo_id=repo_id,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "print(bar(21))\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=OUTPUT_DIR + \"/vae\",\n",
        "    path_in_repo=\"vae\",\n",
        "    repo_id=repo_id,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "print(bar(23))\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=OUTPUT_DIR + \"/model_index.json\",\n",
        "    path_in_repo=\"model_index.json\",\n",
        "    repo_id=repo_id,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "print(bar(25))\n",
        "\n",
        "print(f'The concept was uploaded successfully: https://huggingface.co/{repo_id}')"
      ],
      "metadata": {
        "id": "d67Ppi63We_I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}